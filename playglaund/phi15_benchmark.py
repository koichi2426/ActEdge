from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import subprocess

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model_id = "microsoft/phi-1_5"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "Explain the theory of relativity in simple terms."
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# powermetricsã‚³ãƒãƒ³ãƒ‰ã§æ¸¬å®šé–‹å§‹å‰ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹é›»åŠ›ã‚’å–å¾—
def get_power_snapshot():
    try:
        output = subprocess.check_output(
            ["sudo", "powermetrics", "-n", "1", "-i", "1000", "--samplers", "all"],
            stderr=subprocess.DEVNULL
        ).decode("utf-8")
        for line in output.splitlines():
            if "CPU Power:" in line:
                power_watts = float(line.split(":")[1].strip().split()[0])
                return power_watts
    except Exception as e:
        print("âš ï¸ powermetrics å®Ÿè¡Œå¤±æ•—:", e)
        return None

print("ğŸ”§ é›»åŠ›æ¸¬å®šã«ã¯ sudo æ¨©é™ãŒå¿…è¦ã§ã™ã€‚æœ€åˆã« sudo ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

power_before = get_power_snapshot()
start_time = time.time()
outputs = model.generate(**inputs, max_new_tokens=100)
end_time = time.time()
power_after = get_power_snapshot()

# æ¨è«–æ™‚é–“ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°
elapsed_time = end_time - start_time
generated_ids = outputs[0][inputs['input_ids'].shape[-1]:]
num_generated_tokens = len(generated_ids)

# å¹³å‡é›»åŠ›ã¨æ¨å®šæ¶ˆè²»ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
if power_before is not None and power_after is not None:
    avg_power = (power_before + power_after) / 2
    estimated_energy_j = avg_power * elapsed_time
else:
    avg_power = estimated_energy_j = None

# å‡ºåŠ›ãƒ†ã‚­ã‚¹ãƒˆ
output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(output_text)

# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
with open("phi_output.txt", "w", encoding="utf-8") as f:
    f.write(output_text)

# ãƒ¡ã‚¿æƒ…å ±å‡ºåŠ›
print(f"\nâ± æ¨è«–æ™‚é–“: {elapsed_time:.3f} ç§’")
print(f"ğŸ”¢ ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {num_generated_tokens}")
if num_generated_tokens > 0:
    print(f"âš™ï¸ å¹³å‡: {elapsed_time * 1000 / num_generated_tokens:.2f} ms/token")
if avg_power is not None:
    print(f"âš¡ï¸ å¹³å‡CPUé›»åŠ›: {avg_power:.2f} W")
    print(f"ğŸ”‹ æ¨å®šæ¶ˆè²»ã‚¨ãƒãƒ«ã‚®ãƒ¼: {estimated_energy_j:.2f} J")
else:
    print("âš ï¸ é›»åŠ›æ¸¬å®šã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆpowermetricsæœªè¨±å¯ã¾ãŸã¯å¤±æ•—ï¼‰")
