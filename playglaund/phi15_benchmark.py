from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model_id = "microsoft/phi-1_5"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
prompt = "Explain the theory of relativity in simple terms."

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹
start_time = time.time()

# æ¨è«–
outputs = model.generate(**inputs, max_new_tokens=100)

# æ¨è«–æ™‚é–“ã®è¨ˆæ¸¬çµ‚äº†
end_time = time.time()
elapsed_time = end_time - start_time  # ç§’

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆç®—
generated_ids = outputs[0][inputs['input_ids'].shape[-1]:]  # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†
num_generated_tokens = len(generated_ids)

# å‡ºåŠ›è¡¨ç¤º
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
print(f"\nâ± æ¨è«–æ™‚é–“: {elapsed_time:.3f} ç§’ ({elapsed_time * 1000:.1f} ms)")
print(f"ğŸ”¢ ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {num_generated_tokens}")
if num_generated_tokens > 0:
    print(f"âš™ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šå¹³å‡: {elapsed_time * 1000 / num_generated_tokens:.2f} ms/token")
